import csv
import os
import re
import sys
import subprocess
import tempfile
from datetime import datetime
from pathlib import Path

configfile: os.path.join(workflow.basedir, "cdot_transcripts.yaml")

cdot_json = os.path.join(workflow.basedir, "cdot_json.py")
cdot_dir = os.path.dirname(workflow.basedir)
cdot_output_raw = subprocess.check_output(f"{sys.executable} {cdot_json} --version", shell=True, env={"PYTHONPATH": cdot_dir})
cdot_data_version = cdot_output_raw.decode().strip()

# Load Ensembl -> GENCODE mapping CSV
gencode_csv = os.path.join(workflow.basedir, "ensembl_gencode.csv")
_gencode_map = {}
with open(gencode_csv) as _fh:
    _reader = csv.DictReader(_fh)
    for _r in _reader:
        _gencode_map[_r["Ensembl release"].strip()] = _r["GENCODE release"].strip()

# Regex to pull Ensembl release number from a URL (handles release_XX or release-XX)
_re_ensembl_rel = re.compile(r"release[_-](\d+)")

def _ensembl_release_from_url(url: str):
    m = _re_ensembl_rel.search(url)
    return m.group(1) if m else None

def _gencode_release_from_url(url: str):
    erel = _ensembl_release_from_url(url)
    if erel is None:
        return None
    return _gencode_map.get(erel)

def _gencode_metadata_path(wc):
    # Return path (string) if Ensembl; else [] so Snakemake ignores
    if wc.annotation_consortium.lower() != "ensembl":
        return []
    grel = _gencode_release_from_url(all_urls[wc.name])
    if grel is None:
        print(f"[WARN] No Ensembl -> GENCODE mapping for {wc.name}; skipping GENCODE metadata", file=sys.stderr)
        return []
    return f"downloads/gencode_metadata/gencode.v{grel}.metadata.HGNC.gz"

def _gencode_metadata_url(wc):
    if wc.annotation_consortium.lower() != "ensembl":
        return ""
    grel = _gencode_release_from_url(all_urls[wc.name])
    if grel is None:
        return ""  # Skip download
    return f"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_{grel}/gencode.v{grel}.metadata.HGNC.gz"


# Name it based on date as it may vary
today = datetime.now().date().isoformat()
gene_info_download_filename = os.path.join("downloads/gene_info", f"Homo_sapiens.gene_info.{today}.gz")
gene_info_json_filename = f"Homo_sapiens.gene-info-{cdot_data_version}.{today}.json.gz"

all_urls = {}
name_uta = {}

annotation_consortium_list = []
genome_builds = set()
for annotation_consortium, builds in config["config"].items():
    annotation_consortium_list.append(annotation_consortium)
    for gb, cdot_file_data in builds.items():
        genome_builds.add(gb)
        for name, data in cdot_file_data.items():
            url = None
            if uta := data.get("uta"):
                name_uta[name] = uta
                if schema := uta.get("schema"):
                    url = f"postgresql://uta.biocommons.org/{schema}"
            else:
                url = data.get("url")

            if url:
                all_urls[name] = url

genome_builds = list(sorted(genome_builds))
merged_genome_build_label = "_".join(genome_builds).lower()


def get_url_from_name(wildcards):
    return all_urls[wildcards.name]


def get_cdot_command(wildcards):
    url = all_urls[wildcards.name]
    # gffs can end with 'gff.gz' or 'gff3.gz', gtfs always end with 'gtf.gz'
    cdot_command = "gtf_to_json" if url.endswith(".gtf.gz") else "gff3_to_json"
    return cdot_command


def get_build_input_files(wildcards):
    """ This needs to be in order of the YAML file (as we overwrite as we go) """
    cdot_file_data = config["config"][wildcards.annotation_consortium][wildcards.genome_build]
    prefix = f"{wildcards.annotation_consortium}/{wildcards.genome_build}/cdot-{cdot_data_version}"
    input_files = []
    for name, val in cdot_file_data.items():
        if 'url' in val:
            filename = prefix + f".{name}.json.gz"
        elif 'uta' in val:
            filename = prefix + f"_uta.{name}.json.gz"
        else:
            raise ValueError(f"Don't know how to handle {name}: {val}")
        input_files.append(filename)
    return input_files


rule all:
    input:
        expand("{annotation_consortium}/cdot-" + cdot_data_version + ".all-builds-{annotation_consortium}" + f"-{merged_genome_build_label}.json.gz",
               annotation_consortium=annotation_consortium_list),
        expand("{annotation_consortium}/cdot-" + cdot_data_version + ".{annotation_consortium}.{genome_build}.json.gz",
               annotation_consortium=annotation_consortium_list,
               genome_build=genome_builds)


rule cdot_merge_annotation_consortium:
    input:
        lambda wc: expand("{annotation_consortium}/cdot-" + cdot_data_version + ".{annotation_consortium}.{genome_build}.json.gz",
                          annotation_consortium=[wc.annotation_consortium],
                          genome_build=genome_builds)  # only this consortium
    output:
        "{annotation_consortium}/cdot-" + cdot_data_version + ".all-builds-{annotation_consortium}" + f"-{merged_genome_build_label}.json.gz"
    run:
        build_params_and_match = {
            "grch37": "GRCh37",
            "grch38": "GRCh38",
            "t2t_chm13v2": "T2T-CHM13v2.0",
        }
        build_files = {}
        for input_file in input:
            for build_param, build_match in build_params_and_match.items():
                if build_match in input_file:
                    build_files[build_param] = input_file
        build_params = [f'--{build_param}="{input_file}"' for build_param, input_file in build_files.items()]
        combine_build_cmd = f'PYTHONPATH={cdot_dir} {cdot_json} combine_builds --output "{output}" ' + ' '.join(build_params)
        shell(combine_build_cmd)


rule cdot_merge_historical_json:
    # Merges multiple files together for 1 build
    input:
        get_build_input_files,
    output:
        "{annotation_consortium}/cdot-" + cdot_data_version + ".{annotation_consortium}" + ".{genome_build}.json.gz"
    shell:
        """
            PYTHONPATH={cdot_dir} \
            {cdot_json} \
                merge_historical \
                {input} \
                --genome-build={wildcards.genome_build} \
                --output {output}
        """


rule cdot_gff_json:
    # Individual GFF
    input:
        gene_info_json=gene_info_json_filename,
        gff_file="downloads/{name}.gz",
        gencode_metadata=_gencode_metadata_path  # Optional, only used for Ensembl
    output:
        "{annotation_consortium}/{genome_build}/cdot-" + cdot_data_version + ".{name}.json.gz"
    params:
        url=get_url_from_name,
        cdot_command=get_cdot_command,
        gencode_param=lambda wc: (
            (f'--gencode-hgnc-metadata "{_gencode_metadata_path(wc)}"')
            if (wc.annotation_consortium.lower() == "ensembl" and _gencode_release_from_url(all_urls[wc.name]) is not None)
            else ""
        )
    shell:
        """
            PYTHONPATH={cdot_dir} \
            {cdot_json} \
                {params.cdot_command} \
                "{input.gff_file}" \
                --url "{params.url}" \
                --genome-build="{wildcards.genome_build}" \
                --output "{output}" \
                --gene-info-json="{input.gene_info_json}" \
                {params.gencode_param}
        """


rule download_gff_files:
    threads: 4  # We seem to sometimes get failures/booted if too many simultaneous connections to site
    output:
        # Don't re-download if snakemake script changes
        protected("downloads/{name,[^/]+}.gz")
    params:
        url=lambda wc: all_urls[wc.name]
    shell:
        "curl --fail --show-error -o {output} {params.url}"


# Download GENCODE metadata (HGNC) for Ensembl builds (wildcarded by GENCODE release)
# Triggered only when cdot_gff_json needs the file (i.e., Ensembl).
rule download_gencode_metadata:
    output:
        protected("downloads/gencode_metadata/gencode.v{grel}.metadata.HGNC.gz")
    params:
        url=lambda wc: f"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_{wc.grel}/gencode.v{wc.grel}.metadata.HGNC.gz"
    shell:
        "curl --fail --show-error -o {output} {params.url}"


rule fetch_uta_from_db:
    output:
        protected("uta/{name}.csv")
    params:
        schema=lambda wc: name_uta[wc.name]["schema"],
        template=lambda wc: name_uta[wc.name]["template"]
    run:
        template_file = os.path.join(workflow.basedir, params.template)
        with open(template_file) as f:
            template = f.read()
        sql = template.format(schema=params.schema, output_file=output[0]).replace("\n", " ")

        with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".sql") as tf:
            tf.write(sql)
            tf.flush()
            sql_path = Path(tf.name)
            shell(f'PGPASSWORD=anonymous psql -h uta.biocommons.org -U anonymous -d uta -f "{sql_path}"')


rule cdot_uta_json:
    input:
        gene_info_json=gene_info_json_filename,
        uta_file="uta/{name}.csv"
    output:
        "{annotation_consortium}/{genome_build}/cdot-" + cdot_data_version + "_uta.{name}.json.gz"
    params:
        url=get_url_from_name
    shell:
        """
            PYTHONPATH={cdot_dir} \
            {cdot_json} \
                uta_to_json \
                "{input.uta_file}" \
                --url "{params.url}" \
                --genome-build="{wildcards.genome_build}" \
                --output "{output}"
                # --gene-info-json="{input.gene_info_json}"
        """


rule process_gene_info_json:
    input:
        gene_info_download_filename
    output:
        gene_info_json_filename
    shell:
        """
            PYTHONPATH={cdot_dir} \
            "{workflow.basedir}/cdot_gene_info.py" \
                --gene-info {input} \
                --output {output} \
                --email cdot@cdot.cc
        """


rule download_gene_info:
    output:
        protected(gene_info_download_filename)
    shell:
        "curl --fail --show-error -o {output} https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/Homo_sapiens.gene_info.gz"
