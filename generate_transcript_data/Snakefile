import sys
import subprocess
import tempfile
from datetime import datetime
from pathlib import Path

configfile: os.path.join(workflow.basedir, "cdot_transcripts.yaml")

cdot_json = os.path.join(workflow.basedir, "cdot_json.py")
cdot_dir = os.path.dirname(workflow.basedir)
cdot_output_raw = subprocess.check_output(f"{sys.executable} {cdot_json} --version", shell=True, env={"PYTHONPATH": cdot_dir})
cdot_data_version = cdot_output_raw.decode().strip()

# Name it based on date as it may vary
today = datetime.now().date().isoformat()
gene_info_download_filename = os.path.join("downloads/gene_info", f"Homo_sapiens.gene_info.{today}.gz")
gene_info_json_filename = f"Homo_sapiens.gene-info-{cdot_data_version}.{today}.json.gz"

all_urls = {}
name_uta = {}

annotation_consortium_list = []
genome_builds = set()
for annotation_consortium, builds in config["config"].items():
    annotation_consortium_list.append(annotation_consortium)
    for gb, cdot_file_data in builds.items():
        genome_builds.add(gb)
        for name, data in cdot_file_data.items():
            url = None
            if uta := data.get("uta"):
                name_uta[name] = uta
                if schema := uta.get("schema"):
                    url = f"postgresql://uta.biocommons.org/{schema}"
            else:
                url = data.get("url")

            if url:
                all_urls[name] = url

genome_builds = list(sorted(genome_builds))
merged_genome_build_label = "_".join(genome_builds).lower()


def get_url_from_name(wildcards):
    return all_urls[wildcards.name]


def get_cdot_command(wildcards):
    url = all_urls[wildcards.name]
    # gffs can end with 'gff.gz' or 'gff3.gz', gtfs always end with 'gtf.gz'
    cdot_command = "gtf_to_json" if url.endswith(".gtf.gz") else "gff3_to_json"
    return cdot_command


def get_build_input_files(wildcards):
    """ This needs to be in order of the YAML file (as we overwrite as we go) """
    cdot_file_data = config["config"][wildcards.annotation_consortium][wildcards.genome_build]
    prefix = f"{wildcards.annotation_consortium}/{wildcards.genome_build}/cdot-{cdot_data_version}"
    input_files = []
    for name, val in cdot_file_data.items():
        if 'url' in val:
            filename = prefix + f".{name}.json.gz"
        elif 'uta' in val:
            filename = prefix + f"_uta.{name}.json.gz"
        else:
            raise ValueError(f"Don't know how to handle {name}: {val}")
        input_files.append(filename)
    return input_files


rule all:
    input:
        expand("{annotation_consortium}/cdot-" + cdot_data_version + ".all-builds-{annotation_consortium}" + f"-{merged_genome_build_label}.json.gz",
               annotation_consortium=annotation_consortium_list),
        expand("{annotation_consortium}/cdot-" + cdot_data_version + ".{annotation_consortium}.{genome_build}.json.gz",
               annotation_consortium=annotation_consortium_list,
               genome_build=genome_builds)


rule cdot_merge_annotation_consortium:
    input:
        expand("{annotation_consortium}/cdot-" + cdot_data_version + ".{annotation_consortium}.{genome_build}.json.gz",
               annotation_consortium=annotation_consortium_list,
               genome_build=genome_builds)
    output:
        "{annotation_consortium}/cdot-" + cdot_data_version + ".all-builds-{annotation_consortium}.{merged_genome_build_label}.json.gz"
    run:
        build_params_and_match = {
            "grch37": "GRCh37",
            "grch38": "GRCh38",
            "t2t_chm13v2": "T2T-CHM13v2.0",
        }
        build_files = {}
        for input_file in input:
            for build_param, build_match in build_params_and_match.items():
                if build_match in input_file:
                    build_files[build_param] = input_file
        build_params = [f'--{build_param}="{input_file}"' for build_param, input_file in build_files.items()]
        combine_build_cmd = f'PYTHONPATH={cdot_dir} {cdot_json} combine_builds --output "{output}" ' + ' '.join(build_params)
        shell(combine_build_cmd)


rule cdot_merge_historical_json:
    # Merges multiple files together for 1 build
    input:
        get_build_input_files,
    output:
        "{annotation_consortium}/cdot-" + cdot_data_version + ".{annotation_consortium}" + ".{genome_build}.json.gz"
    shell:
        """
            PYTHONPATH={cdot_dir} \
            {cdot_json} \
                merge_historical \
                {input} \
                --genome-build={wildcards.genome_build} \
                --output {output}
        """


rule cdot_gff_json:
    # Individual GFF
    input:
        gene_info_json=gene_info_json_filename,
        gff_file="downloads/{name}.gz"
    output:
        "{annotation_consortium}/{genome_build}/cdot-" + cdot_data_version + ".{name}.json.gz"
    params:
        url=get_url_from_name,
        cdot_command=get_cdot_command
    shell:
        """
            PYTHONPATH={cdot_dir} \
            {cdot_json} \
                {params.cdot_command} \
                "{input.gff_file}" \
                --url "{params.url}" \
                --genome-build="{wildcards.genome_build}" \
                --output "{output}" \
                --gene-info-json="{input.gene_info_json}"
        """


rule download_gff_files:
    threads: 4  # We seem to sometimes get failures/booted if too many simultaneous connections to site
    output:
        # Don't re-download if snakemake script changes
        protected("downloads/{name}.gz")
    params:
        url=lambda wc: all_urls[wc.name]
    shell:
        "curl --fail --show-error -o {output} {params.url}"


rule fetch_uta_from_db:
    output:
        protected("uta/{name}.csv")
    params:
        schema=lambda wc: name_uta[wc.name]["schema"],
        template=lambda wc: name_uta[wc.name]["template"],
    run:
        template_file = os.path.join(workflow.basedir, params.template)
        with open(template_file) as f:
            template = f.read()
        sql = template.format(schema=params.schema, output_file=output[0]).replace("\n", " ")

        with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".sql") as tf:
            tf.write(sql)
            tf.flush()
            sql_path = Path(tf.name)
            shell(f'PGPASSWORD=anonymous psql -h uta.biocommons.org -U anonymous -d uta -f "{sql_path}"')


rule cdot_uta_json:
    input:
        gene_info_json=gene_info_json_filename,
        uta_file="uta/{name}.csv"
    output:
        "{annotation_consortium}/{genome_build}/cdot-" + cdot_data_version + "_uta.{name}.json.gz"
    params:
        url=get_url_from_name,
    shell:
        """
            PYTHONPATH={cdot_dir} \
            {cdot_json} \
                uta_to_json \
                "{input.uta_file}" \
                --url "{params.url}" \
                --genome-build="{wildcards.genome_build}" \
                --output "{output}"
                # --gene-info-json="{input.gene_info_json}"
        """


rule process_gene_info_json:
    input:
        gene_info_download_filename
    output:
        gene_info_json_filename
    shell:
        """
            PYTHONPATH={cdot_dir} \
            "{workflow.basedir}/cdot_gene_info.py" \
                --gene-info {input} \
                --output {output} \
                --email cdot@cdot.cc
        """


rule download_gene_info:
    output:
        protected(gene_info_download_filename)
    shell:
        "curl --fail --show-error -o {output} https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/Homo_sapiens.gene_info.gz"

